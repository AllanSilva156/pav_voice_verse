{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "4NMg3leDxW8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS==0.15.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M9a6KEWmOiDq",
        "outputId": "8d0254be-98a4-4142-a1bf-933909a10ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting TTS==0.15.5\n",
            "  Downloading TTS-0.15.5-cp310-cp310-manylinux1_x86_64.whl (762 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m762.2/762.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython==0.29.30 (from TTS==0.15.5)\n",
            "  Downloading Cython-0.29.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (2.0.2+cu118)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (0.12.1)\n",
            "Requirement already satisfied: librosa==0.10.0.* in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (0.10.0.post2)\n",
            "Collecting inflect==5.6.0 (from TTS==0.15.5)\n",
            "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (4.66.0)\n",
            "Collecting anyascii (from TTS==0.15.5)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (3.8.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (23.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (2.2.5)\n",
            "Collecting pysbd (from TTS==0.15.5)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn==0.5.1 (from TTS==0.15.5)\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (3.7.1)\n",
            "Collecting trainer (from TTS==0.15.5)\n",
            "  Downloading trainer-0.0.30-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coqpit>=0.0.16 (from TTS==0.15.5)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (0.42.1)\n",
            "Collecting pypinyin (from TTS==0.15.5)\n",
            "  Downloading pypinyin-0.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mecab-python3==1.0.6 (from TTS==0.15.5)\n",
            "  Downloading mecab_python3-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite==1.0.8 (from TTS==0.15.5)\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut[de,es,fr]==2.2.3 (from TTS==0.15.5)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS==0.15.5)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS==0.15.5) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS==0.15.5)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Collecting bangla==0.0.2 (from TTS==0.15.5)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting bnnumerizer (from TTS==0.15.5)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer==0.1.1 (from TTS==0.15.5)\n",
            "  Downloading bnunicodenormalizer-0.1.1.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting k-diffusion (from TTS==0.15.5)\n",
            "  Downloading k_diffusion-0.0.16-py3-none-any.whl (25 kB)\n",
            "Collecting einops (from TTS==0.15.5)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from TTS==0.15.5)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting encodec (from TTS==0.15.5)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.22.0 (from TTS==0.15.5)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba==0.57.0 (from TTS==0.15.5)\n",
            "  Downloading numba-0.57.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de,es,fr]==2.2.3->TTS==0.15.5) (2.12.1)\n",
            "Collecting dateparser~=1.1.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words<1.0.0,>=0.5.10 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa==0.10.0.* (from TTS==0.15.5)\n",
            "  Downloading librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (4.7.1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS==0.15.5) (1.0.5)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba==0.57.0->TTS==0.15.5)\n",
            "  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent>=0.5 (from umap-learn==0.5.1->TTS==0.15.5)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->TTS==0.15.5) (1.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS==0.15.5) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS==0.15.5) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS==0.15.5) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS==0.15.5) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS==0.15.5) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS==0.15.5) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS==0.15.5) (1.3.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->TTS==0.15.5) (2.3.6)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS==0.15.5) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS==0.15.5) (8.1.6)\n",
            "Collecting accelerate (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clean-fid (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
            "Collecting clip-anytorch (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonmerge (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading jsonmerge-1.9.2-py3-none-any.whl (19 kB)\n",
            "Collecting kornia (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from k-diffusion->TTS==0.15.5) (9.4.0)\n",
            "Collecting resize-right (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from k-diffusion->TTS==0.15.5) (0.19.3)\n",
            "Collecting torchdiffeq (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Collecting torchsde (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from k-diffusion->TTS==0.15.5) (0.15.2+cu118)\n",
            "Collecting wandb (from k-diffusion->TTS==0.15.5)\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS==0.15.5) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->TTS==0.15.5) (2023.6.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->TTS==0.15.5) (2023.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer->TTS==0.15.5) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer->TTS==0.15.5) (2.12.3)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->TTS==0.15.5)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->TTS==0.15.5) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->TTS==0.15.5)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->TTS==0.15.5)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->TTS==0.15.5) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut[de,es,fr]==2.2.3->TTS==0.15.5) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->TTS==0.15.5) (2.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut[de,es,fr]==2.2.3->TTS==0.15.5) (1.16.0)\n",
            "Collecting docopt>=0.6.2 (from num2words<1.0.0,>=0.5.10->gruut[de,es,fr]==2.2.3->TTS==0.15.5)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.10.0.*->TTS==0.15.5) (1.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->TTS==0.15.5) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->TTS==0.15.5) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->TTS==0.15.5) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS==0.15.5) (3.2.0)\n",
            "Collecting ftfy (from clip-anytorch->k-diffusion->TTS==0.15.5)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonmerge->k-diffusion->TTS==0.15.5) (4.19.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion->TTS==0.15.5) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion->TTS==0.15.5) (2023.7.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion->TTS==0.15.5) (1.4.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->TTS==0.15.5) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer->TTS==0.15.5) (0.41.1)\n",
            "Collecting boltons>=20.2.1 (from torchsde->k-diffusion->TTS==0.15.5)\n",
            "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trampoline>=0.1.2 (from torchsde->k-diffusion->TTS==0.15.5)\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS==0.15.5) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS==0.15.5) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->trainer->TTS==0.15.5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer->TTS==0.15.5) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.5) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.5) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.5) (0.9.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip-anytorch->k-diffusion->TTS==0.15.5) (0.2.6)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->k-diffusion->TTS==0.15.5)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer->TTS==0.15.5) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->trainer->TTS==0.15.5) (3.2.2)\n",
            "Building wheels for collected packages: bnunicodenormalizer, umap-learn, unidic-lite, bnnumerizer, encodec, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr, pynndescent, gruut, docopt, pathtools\n",
            "  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.1-py3-none-any.whl size=21894 sha256=3048c5d57999ed15c8472bc239196fd66d63124fda7251b48ab3461991e9ac90\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/f6/01/9e68ecec7c7ea85fc9431cfac42eba1c5a5f6debe5070de5c7\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76543 sha256=c780e902e2d06b17cd1425ff5c928148db579812737f0bbca26c3cb6499c3c70\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/21/8e/802cb9c4c606a67139f538cb17bf3bf1b98b739a7900469953\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=f69e8ae717e122cde6f2eeb63ffecea3ee98014ac4ca9defbfab7c2574b7a4a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5260 sha256=de3e27b2ec8de2c3df28bfde39e3c7203cd8d8e4690033db2cb718b17a180845\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=8537c6f66ede2a05f68f089204435da2134edf81b3517a58e3a2542458bb9162\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104870 sha256=add9c9b53da331e3e823c03d969a29e42403b0fa316373c2da441f236e829448\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498180 sha256=dfa7f84ae0b98985d4c111ce7deaebda5d3f290452784fbcbd5bc95791a16339\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297179 sha256=49336ada1b7269b4eaac37422579fe09a28915420873bae11eb3484dafb0f5ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173796 sha256=dba1c70bfc874563c18f3208e76a4bde8b440ae60ab994d09eec443d7964626b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/0a/90/788d92c07744b329b9283e37b29b064f5db6b1bb0442a1a19b\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=5ae081208ab116a180ac695f712d9038795161d40570866d3c989ff9b6cc33d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55617 sha256=ed016944ca5077284d210561f541752723c02c30bf08bd5b70cb73606433f143\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75793 sha256=8ca6f0d3b407dec7e30cbe4a0a5b2ce9f241007f1334f9fc751683f983ff2f49\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5c7de4b259d3f30a477681a54bd96ee736ba09b66c2b7555e4eacc8614ebc253\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=aa9998517f3139fcbe1cd6cf8bc8659c7dd8e31b5924024b4c4396551c806ae7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built bnunicodenormalizer umap-learn unidic-lite bnnumerizer encodec gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr pynndescent gruut docopt pathtools\n",
            "Installing collected packages: unidic-lite, trampoline, tokenizers, safetensors, resize-right, python-crfsuite, pathtools, mecab-python3, jamo, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, boltons, bnunicodenormalizer, bnnumerizer, bangla, smmap, setproctitle, sentry-sdk, pysbd, pypinyin, numpy, num2words, networkx, llvmlite, jsonlines, inflect, gruut-ipa, ftfy, einops, docker-pycreds, cython, coqpit, anyascii, numba, huggingface-hub, gitdb, g2pkk, dateparser, transformers, gruut, GitPython, wandb, pynndescent, librosa, jsonmerge, umap-learn, torchsde, torchdiffeq, kornia, clip-anytorch, clean-fid, accelerate, trainer, k-diffusion, encodec, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 7.0.0\n",
            "    Uninstalling inflect-7.0.0:\n",
            "      Successfully uninstalled inflect-7.0.0\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 0.29.36\n",
            "    Uninstalling Cython-0.29.36:\n",
            "      Successfully uninstalled Cython-0.29.36\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0.post2\n",
            "    Uninstalling librosa-0.10.0.post2:\n",
            "      Successfully uninstalled librosa-0.10.0.post2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.12.2 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.32 TTS-0.15.5 accelerate-0.21.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.1 boltons-23.0.0 clean-fid-0.1.35 clip-anytorch-2.5.2 coqpit-0.0.17 cython-0.29.30 dateparser-1.1.8 docker-pycreds-0.4.0 docopt-0.6.2 einops-0.6.1 encodec-0.1.1 ftfy-6.1.1 g2pkk-0.1.2 gitdb-4.0.10 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 gruut_lang_es-2.0.0 gruut_lang_fr-2.0.2 huggingface-hub-0.16.4 inflect-5.6.0 jamo-0.4.1 jsonlines-1.2.0 jsonmerge-1.9.2 k-diffusion-0.0.16 kornia-0.7.0 librosa-0.10.0 llvmlite-0.40.1 mecab-python3-1.0.6 networkx-2.8.8 num2words-0.5.12 numba-0.57.0 numpy-1.22.0 pathtools-0.1.2 pynndescent-0.5.10 pypinyin-0.49.0 pysbd-0.3.4 python-crfsuite-0.9.9 resize-right-0.0.2 safetensors-0.3.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 torchdiffeq-0.2.3 torchsde-0.2.5 trainer-0.0.30 trampoline-0.1.2 transformers-4.31.0 umap-learn-0.5.1 unidic-lite-1.0.8 wandb-0.15.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from TTS.config.shared_configs import BaseAudioConfig, BaseDatasetConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vNJcNs0TNk9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base de dados"
      ],
      "metadata": {
        "id": "B7o4oxV5xY1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNiNndbSxF4H",
        "outputId": "3ec22b40-bacd-4ebc-94d0-126351ff809a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading gneutralspeech.zip to /content\n",
            "100% 4.75G/4.76G [00:47<00:00, 183MB/s]\n",
            "100% 4.76G/4.76G [00:47<00:00, 108MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Drive para chave API do Kaggle\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "kaggle_json_file = 'kaggle.json'\n",
        "\n",
        "# Download do dataset diretamente do Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"{drive_path}/{kaggle_json_file}\" ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d mediatechlab/gneutralspeech\n",
        "\n",
        "zip_file = 'gneutralspeech.zip'\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "os.remove(zip_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo CSV\n",
        "df_voz = pd.read_csv(\"/content/voz_base_44kHz_16bit/metadata_voz_base_norm.csv\", delimiter=\"|\", header=None, engine='python', error_bad_lines=False)\n",
        "\n",
        "# Criar um novo dataframe usando as colunas desejadas e separando-as com \" | \"\n",
        "metadata_voz_df = df_voz[[0, 1]].apply(lambda x: ' | '.join(x.astype(str)), axis=1)\n",
        "\n",
        "# Salvar o dataframe no formato de arquivo .txt\n",
        "with open('/content/metadata_voz.txt', 'w') as f:\n",
        "    for line in metadata_voz_df:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "files_path = [filename.split('/')[-1].split('.')[0] for filename in glob.glob('/content/voz_base_44kHz_16bit/wavs/*.wav')]\n",
        "lines_list = []\n",
        "\n",
        "# Formatar os nomes dos arquivos na base de metadados\n",
        "with open('/content/metadata_voz.txt', 'r') as metadata:\n",
        "  for line in metadata.readlines():\n",
        "    filename, txt = line.strip().split('|')\n",
        "    filename = filename.strip()\n",
        "    txt = txt.strip()\n",
        "    if filename in files_path:\n",
        "      lines_list.append('|'.join([filename, txt])+'\\n')\n",
        "\n",
        "# Escrever um novo arquivo de metadado corrigido\n",
        "with open('/content/metadata.txt', 'w') as new_metadata:\n",
        "  for line in lines_list[:800]:\n",
        "    new_metadata.write(line)"
      ],
      "metadata": {
        "id": "BPEWPWK4Zh3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações de dataset\n",
        "dataset_config = BaseDatasetConfig(formatter='thorsten', meta_file_train=\"/content/metadata.txt\", path=\"/content/voz_base_44kHz_16bit\")\n",
        "\n",
        "# Diretório de saída\n",
        "output_path = '/content/output'\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ],
      "metadata": {
        "id": "LAjtdSgFasn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento do GlowTTS (modelo acústico)"
      ],
      "metadata": {
        "id": "TBQyfwBzxbVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações dos áudios\n",
        "audio_config = BaseAudioConfig(\n",
        "    sample_rate=44100,\n",
        "    do_trim_silence=True,\n",
        "    trim_db=60.0,\n",
        "    signal_norm=False,\n",
        "    mel_fmin=0.0,\n",
        "    mel_fmax=8000,\n",
        "    spec_gain=1.0,\n",
        "    log_func=\"np.log\",\n",
        "    ref_level_db=20,\n",
        "    preemphasis=0.0,\n",
        ")\n",
        "\n",
        "# Parâmetros para o treinamento do  modelo\n",
        "config = GlowTTSConfig(\n",
        "    audio=audio_config,\n",
        "    batch_size=16,\n",
        "    eval_batch_size=1,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=100,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"pt\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=50,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        ")\n",
        "\n",
        "# Pré-processamento dos áudios\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "\n",
        "# Tokenização\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# Carregamento dos dados de treino e validação\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "# Inicialização do modelo\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n",
        "\n",
        "# Treinamento do modelo\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model,\n",
        "    train_samples=train_samples, eval_samples=eval_samples\n",
        ")\n",
        "trainer.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_uJd9E8TwBY",
        "outputId": "f755e3dc-f907-4d63-9324-9d6a12ddd2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:44100\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:60.0\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " | > Found 800 files in /content/voz_base_44kHz_16bit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: True\n",
            " | > Precision: fp16\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 54321\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            " > Start Tensorboard: tensorboard --logdir=/content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            " > Model has 28610257 parameters\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Pre-computing phonemes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 317/792 [00:04<00:07, 65.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". . , , , … .\n",
            " [!] Character '…' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 542/792 [00:07<00:03, 70.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", ,  — , , .\n",
            " [!] Character '—' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 792/792 [00:10<00:00, 72.70it/s]\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:43:35) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: pt\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 2 not found characters:\n",
            "\t| > …\n",
            "\t| > —\n",
            "| > Number of instances : 792\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 286\n",
            " | > Min text length: 82\n",
            " | > Avg text length: 188.57449494949495\n",
            " | \n",
            " | > Max audio length: 1257063.0\n",
            " | > Min audio length: 226548.0\n",
            " | > Avg audio length: 534580.0\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:43:56 -- STEP: 0/50 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 14.4543  (14.454307556152344)\n",
            "     | > loader_time: 5.7283  (5.728253602981567)\n",
            "\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            " [!] `train_step()` retuned `None` outputs. Skipping training step.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: pt\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 2 not found characters:\n",
            "\t| > …\n",
            "\t| > —\n",
            "| > Number of instances : 8\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 236\n",
            " | > Min text length: 99\n",
            " | > Avg text length: 179.0\n",
            " | \n",
            " | > Max audio length: 634456.0\n",
            " | > Min audio length: 291262.0\n",
            " | > Avg audio length: 507695.625\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.001940420695713588 \u001b[0m(+0)\n",
            "     | > avg_loss: 26.46731458391462 \u001b[0m(+0)\n",
            "     | > avg_log_mle: 1.0781050409589494 \u001b[0m(+0)\n",
            "     | > avg_loss_dur: 25.38920933859689 \u001b[0m(+0)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.001940420695713588 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 26.46731458391462 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0781050409589494 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 25.38920933859689 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_50.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:45:58) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:46:05 -- STEP: 0/50 -- GLOBAL_STEP: 50\u001b[0m\n",
            "     | > loss: 26.65632438659668  (26.65632438659668)\n",
            "     | > log_mle: 1.0921236276626587  (1.0921236276626587)\n",
            "     | > loss_dur: 25.56420135498047  (25.56420135498047)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(29.6717, device='cuda:0')  (tensor(29.6717, device='cuda:0'))\n",
            "     | > current_lr: 2.5e-07 \n",
            "     | > step_time: 3.2591  (3.2590811252593994)\n",
            "     | > loader_time: 3.6685  (3.6685452461242676)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0026228087288992746 \u001b[0m(+0.0006823880331856865)\n",
            "     | > avg_loss:\u001b[92m 26.44022137778146 \u001b[0m(-0.027093206133162795)\n",
            "     | > avg_log_mle:\u001b[92m 1.0778675930840629 \u001b[0m(-0.00023744787488655383)\n",
            "     | > avg_loss_dur:\u001b[92m 25.362354006086075 \u001b[0m(-0.026855332510812957)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0026228087288992746 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 26.44022137778146 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0778675930840629 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 25.362354006086075 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_100.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:47:54) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:47:59 -- STEP: 0/50 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss: 23.457963943481445  (23.457963943481445)\n",
            "     | > log_mle: 1.0918407440185547  (1.0918407440185547)\n",
            "     | > loss_dur: 22.36612319946289  (22.36612319946289)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(26.0661, device='cuda:0')  (tensor(26.0661, device='cuda:0'))\n",
            "     | > current_lr: 5e-07 \n",
            "     | > step_time: 1.5507  (1.550727128982544)\n",
            "     | > loader_time: 3.1996  (3.1995561122894287)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00258599008832659 \u001b[0m(-3.68186405726844e-05)\n",
            "     | > avg_loss:\u001b[91m 27.67368643624442 \u001b[0m(+1.2334650584629614)\n",
            "     | > avg_log_mle:\u001b[92m 1.077216420854841 \u001b[0m(-0.0006511722292219524)\n",
            "     | > avg_loss_dur:\u001b[91m 26.596469742911204 \u001b[0m(+1.234115736825128)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.00258599008832659 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 27.67368643624442 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.077216420854841 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 26.596469742911204 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:49:49) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:49:54 -- STEP: 0/50 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss: 25.875324249267578  (25.875324249267578)\n",
            "     | > log_mle: 1.0915299654006958  (1.0915299654006958)\n",
            "     | > loss_dur: 24.783794403076172  (24.783794403076172)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(28.5940, device='cuda:0')  (tensor(28.5940, device='cuda:0'))\n",
            "     | > current_lr: 7.5e-07 \n",
            "     | > step_time: 1.627  (1.6270287036895752)\n",
            "     | > loader_time: 3.3132  (3.3132264614105225)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002007722854614258 \u001b[0m(-0.0005782672337123324)\n",
            "     | > avg_loss:\u001b[92m 27.63676275525774 \u001b[0m(-0.03692368098667842)\n",
            "     | > avg_log_mle:\u001b[92m 1.0760207516806466 \u001b[0m(-0.001195669174194336)\n",
            "     | > avg_loss_dur:\u001b[92m 26.5607419695173 \u001b[0m(-0.03572777339390498)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002007722854614258 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 27.63676275525774 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0760207516806466 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 26.5607419695173 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:51:39) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:51:47 -- STEP: 0/50 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss: 26.901220321655273  (26.901220321655273)\n",
            "     | > log_mle: 1.0902998447418213  (1.0902998447418213)\n",
            "     | > loss_dur: 25.81092071533203  (25.81092071533203)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(30.4851, device='cuda:0')  (tensor(30.4851, device='cuda:0'))\n",
            "     | > current_lr: 1e-06 \n",
            "     | > step_time: 2.7171  (2.7171413898468018)\n",
            "     | > loader_time: 4.6718  (4.671815872192383)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0038066591535295758 \u001b[0m(+0.001798936298915318)\n",
            "     | > avg_loss:\u001b[91m 28.89525740487235 \u001b[0m(+1.258494649614608)\n",
            "     | > avg_log_mle:\u001b[92m 1.0741172177450997 \u001b[0m(-0.001903533935546875)\n",
            "     | > avg_loss_dur:\u001b[91m 27.82114028930664 \u001b[0m(+1.260398319789342)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0038066591535295758 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 28.89525740487235 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0741172177450997 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 27.82114028930664 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:53:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:53:40 -- STEP: 0/50 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss: 26.221940994262695  (26.221940994262695)\n",
            "     | > log_mle: 1.0884785652160645  (1.0884785652160645)\n",
            "     | > loss_dur: 25.13346290588379  (25.13346290588379)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(29.5680, device='cuda:0')  (tensor(29.5680, device='cuda:0'))\n",
            "     | > current_lr: 1.2499999999999999e-06 \n",
            "     | > step_time: 2.2929  (2.2929325103759766)\n",
            "     | > loader_time: 3.4211  (3.4210755825042725)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0031122139522007535 \u001b[0m(-0.0006944452013288222)\n",
            "     | > avg_loss:\u001b[91m 29.2554383959089 \u001b[0m(+0.3601809910365503)\n",
            "     | > avg_log_mle:\u001b[92m 1.071187973022461 \u001b[0m(-0.0029292447226387797)\n",
            "     | > avg_loss_dur:\u001b[91m 28.184250695364817 \u001b[0m(+0.36311040605817624)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0031122139522007535 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 29.2554383959089 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.071187973022461 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 28.184250695364817 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:55:29) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:55:34 -- STEP: 0/50 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss: 25.302392959594727  (25.302392959594727)\n",
            "     | > log_mle: 1.085363745689392  (1.085363745689392)\n",
            "     | > loss_dur: 24.217029571533203  (24.217029571533203)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(28.3146, device='cuda:0')  (tensor(28.3146, device='cuda:0'))\n",
            "     | > current_lr: 1.5e-06 \n",
            "     | > step_time: 1.3952  (1.3952107429504395)\n",
            "     | > loader_time: 3.4002  (3.400228261947632)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0028086730412074496 \u001b[0m(-0.00030354091099330394)\n",
            "     | > avg_loss:\u001b[91m 30.135390417916433 \u001b[0m(+0.879952022007533)\n",
            "     | > avg_log_mle:\u001b[92m 1.0666626521519251 \u001b[0m(-0.0045253208705358094)\n",
            "     | > avg_loss_dur:\u001b[91m 29.068728038242885 \u001b[0m(+0.8844773428780677)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0028086730412074496 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 30.135390417916433 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0666626521519251 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 29.068728038242885 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 7/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:57:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:57:30 -- STEP: 0/50 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss: 25.973363876342773  (25.973363876342773)\n",
            "     | > log_mle: 1.0811514854431152  (1.0811514854431152)\n",
            "     | > loss_dur: 24.8922119140625  (24.8922119140625)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(29.3616, device='cuda:0')  (tensor(29.3616, device='cuda:0'))\n",
            "     | > current_lr: 1.75e-06 \n",
            "     | > step_time: 1.5637  (1.5637295246124268)\n",
            "     | > loader_time: 4.8027  (4.802672386169434)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0020201546805245535 \u001b[0m(-0.0007885183606828961)\n",
            "     | > avg_loss:\u001b[92m 29.76131166730608 \u001b[0m(-0.37407875061035156)\n",
            "     | > avg_log_mle:\u001b[92m 1.0597516298294067 \u001b[0m(-0.00691102232251839)\n",
            "     | > avg_loss_dur:\u001b[92m 28.70155988420759 \u001b[0m(-0.36716815403529424)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0020201546805245535 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 29.76131166730608 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0597516298294067 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 28.70155988420759 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 8/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-12 23:59:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-12 23:59:24 -- STEP: 0/50 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss: 25.86806869506836  (25.86806869506836)\n",
            "     | > log_mle: 1.0744010210037231  (1.0744010210037231)\n",
            "     | > loss_dur: 24.79366683959961  (24.79366683959961)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(28.7740, device='cuda:0')  (tensor(28.7740, device='cuda:0'))\n",
            "     | > current_lr: 2e-06 \n",
            "     | > step_time: 2.7491  (2.7491183280944824)\n",
            "     | > loader_time: 5.2578  (5.257805347442627)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0026419503348214285 \u001b[0m(+0.000621795654296875)\n",
            "     | > avg_loss:\u001b[92m 29.493578229631698 \u001b[0m(-0.2677334376743836)\n",
            "     | > avg_log_mle:\u001b[92m 1.0496142762047904 \u001b[0m(-0.010137353624616319)\n",
            "     | > avg_loss_dur:\u001b[92m 28.443963732038224 \u001b[0m(-0.2575961521693664)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0026419503348214285 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 29.493578229631698 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0496142762047904 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 28.443963732038224 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 9/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:01:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:01:13 -- STEP: 0/50 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss: 26.564054489135742  (26.564054489135742)\n",
            "     | > log_mle: 1.0640860795974731  (1.0640860795974731)\n",
            "     | > loss_dur: 25.499967575073242  (25.499967575073242)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(28.8348, device='cuda:0')  (tensor(28.8348, device='cuda:0'))\n",
            "     | > current_lr: 2.25e-06 \n",
            "     | > step_time: 2.1682  (2.1681630611419678)\n",
            "     | > loader_time: 3.2103  (3.2102863788604736)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.003102166312081473 \u001b[0m(+0.0004602159772600445)\n",
            "     | > avg_loss:\u001b[92m 28.93283544267927 \u001b[0m(-0.560742786952428)\n",
            "     | > avg_log_mle:\u001b[92m 1.0354631458009993 \u001b[0m(-0.014151130403791123)\n",
            "     | > avg_loss_dur:\u001b[92m 27.89737238202776 \u001b[0m(-0.5465913500104627)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.003102166312081473 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 28.93283544267927 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0354631458009993 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 27.89737238202776 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 10/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:03:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:03:07 -- STEP: 0/50 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss: 26.48465347290039  (26.48465347290039)\n",
            "     | > log_mle: 1.0490944385528564  (1.0490944385528564)\n",
            "     | > loss_dur: 25.435558319091797  (25.435558319091797)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(27.9425, device='cuda:0')  (tensor(27.9425, device='cuda:0'))\n",
            "     | > current_lr: 2.4999999999999998e-06 \n",
            "     | > step_time: 1.4742  (1.4742381572723389)\n",
            "     | > loader_time: 3.1912  (3.191190481185913)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0029093538011823383 \u001b[0m(-0.00019281251089913473)\n",
            "     | > avg_loss:\u001b[92m 28.559900556291854 \u001b[0m(-0.3729348863874158)\n",
            "     | > avg_log_mle:\u001b[92m 1.0167917353766305 \u001b[0m(-0.018671410424368817)\n",
            "     | > avg_loss_dur:\u001b[92m 27.543108531406947 \u001b[0m(-0.3542638506208142)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0029093538011823383 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 28.559900556291854 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 1.0167917353766305 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 27.543108531406947 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 11/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:04:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:05:02 -- STEP: 0/50 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss: 26.02890968322754  (26.02890968322754)\n",
            "     | > log_mle: 1.0298669338226318  (1.0298669338226318)\n",
            "     | > loss_dur: 24.999042510986328  (24.999042510986328)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(26.4876, device='cuda:0')  (tensor(26.4876, device='cuda:0'))\n",
            "     | > current_lr: 2.75e-06 \n",
            "     | > step_time: 1.6528  (1.6527626514434814)\n",
            "     | > loader_time: 3.1474  (3.147446632385254)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002599409648350307 \u001b[0m(-0.00030994415283203125)\n",
            "     | > avg_loss:\u001b[92m 28.556949615478516 \u001b[0m(-0.002950940813338576)\n",
            "     | > avg_log_mle:\u001b[92m 0.9942120909690857 \u001b[0m(-0.022579644407544786)\n",
            "     | > avg_loss_dur:\u001b[91m 27.562737601143972 \u001b[0m(+0.019629069737025162)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002599409648350307 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 28.556949615478516 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.9942120909690857 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 27.562737601143972 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 12/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:06:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:06:57 -- STEP: 0/50 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss: 25.987871170043945  (25.987871170043945)\n",
            "     | > log_mle: 1.006934404373169  (1.006934404373169)\n",
            "     | > loss_dur: 24.98093605041504  (24.98093605041504)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(25.3429, device='cuda:0')  (tensor(25.3429, device='cuda:0'))\n",
            "     | > current_lr: 3e-06 \n",
            "     | > step_time: 1.6157  (1.6157341003417969)\n",
            "     | > loader_time: 5.1867  (5.18671441078186)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002511092594691685 \u001b[0m(-8.83170536586219e-05)\n",
            "     | > avg_loss:\u001b[92m 28.106936591012136 \u001b[0m(-0.45001302446637936)\n",
            "     | > avg_log_mle:\u001b[92m 0.9704767976488385 \u001b[0m(-0.023735293320247153)\n",
            "     | > avg_loss_dur:\u001b[92m 27.136460168021067 \u001b[0m(-0.42627743312290534)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002511092594691685 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 28.106936591012136 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.9704767976488385 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 27.136460168021067 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 13/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:08:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:08:48 -- STEP: 0/50 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss: 25.689516067504883  (25.689516067504883)\n",
            "     | > log_mle: 0.9815424084663391  (0.9815424084663391)\n",
            "     | > loss_dur: 24.70797348022461  (24.70797348022461)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(24.0532, device='cuda:0')  (tensor(24.0532, device='cuda:0'))\n",
            "     | > current_lr: 3.25e-06 \n",
            "     | > step_time: 2.48  (2.480015754699707)\n",
            "     | > loader_time: 5.2468  (5.246845483779907)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0021647044590541293 \u001b[0m(-0.00034638813563755587)\n",
            "     | > avg_loss:\u001b[92m 27.357282638549805 \u001b[0m(-0.7496539524623316)\n",
            "     | > avg_log_mle:\u001b[92m 0.9473785417420524 \u001b[0m(-0.023098255906786158)\n",
            "     | > avg_loss_dur:\u001b[92m 26.40990420750209 \u001b[0m(-0.7265559605189758)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0021647044590541293 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 27.357282638549805 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.9473785417420524 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 26.40990420750209 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 14/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:10:31) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:10:36 -- STEP: 0/50 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss: 25.060739517211914  (25.060739517211914)\n",
            "     | > log_mle: 0.9576022624969482  (0.9576022624969482)\n",
            "     | > loss_dur: 24.103137969970703  (24.103137969970703)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(23.4337, device='cuda:0')  (tensor(23.4337, device='cuda:0'))\n",
            "     | > current_lr: 3.5e-06 \n",
            "     | > step_time: 1.4817  (1.4817216396331787)\n",
            "     | > loader_time: 3.5025  (3.502476930618286)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0027330262320382254 \u001b[0m(+0.0005683217729840961)\n",
            "     | > avg_loss:\u001b[92m 26.731351579938615 \u001b[0m(-0.6259310586111901)\n",
            "     | > avg_log_mle:\u001b[92m 0.924613322530474 \u001b[0m(-0.02276521921157837)\n",
            "     | > avg_loss_dur:\u001b[92m 25.806738444737025 \u001b[0m(-0.603165762765066)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0027330262320382254 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 26.731351579938615 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.924613322530474 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 25.806738444737025 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 15/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:12:24) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:12:29 -- STEP: 0/50 -- GLOBAL_STEP: 750\u001b[0m\n",
            "     | > loss: 24.46186637878418  (24.46186637878418)\n",
            "     | > log_mle: 0.9324480295181274  (0.9324480295181274)\n",
            "     | > loss_dur: 23.5294189453125  (23.5294189453125)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(22.0217, device='cuda:0')  (tensor(22.0217, device='cuda:0'))\n",
            "     | > current_lr: 3.7499999999999997e-06 \n",
            "     | > step_time: 1.5865  (1.5865161418914795)\n",
            "     | > loader_time: 3.5777  (3.5776820182800293)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.002762658255440848 \u001b[0m(+2.9632023402622644e-05)\n",
            "     | > avg_loss:\u001b[92m 26.088875361851283 \u001b[0m(-0.6424762180873316)\n",
            "     | > avg_log_mle:\u001b[92m 0.9010975786617824 \u001b[0m(-0.023515743868691596)\n",
            "     | > avg_loss_dur:\u001b[92m 25.187778200422013 \u001b[0m(-0.6189602443150122)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002762658255440848 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 26.088875361851283 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.9010975786617824 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 25.187778200422013 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_800.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 16/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:14:17) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:14:25 -- STEP: 0/50 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > loss: 23.470014572143555  (23.470014572143555)\n",
            "     | > log_mle: 0.9090144634246826  (0.9090144634246826)\n",
            "     | > loss_dur: 22.56100082397461  (22.56100082397461)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(21.2025, device='cuda:0')  (tensor(21.2025, device='cuda:0'))\n",
            "     | > current_lr: 4e-06 \n",
            "     | > step_time: 2.1651  (2.1650991439819336)\n",
            "     | > loader_time: 5.2746  (5.274641990661621)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002036571502685547 \u001b[0m(-0.0007260867527553012)\n",
            "     | > avg_loss:\u001b[92m 25.512663432529994 \u001b[0m(-0.5762119293212891)\n",
            "     | > avg_log_mle:\u001b[92m 0.873758784362248 \u001b[0m(-0.027338794299534452)\n",
            "     | > avg_loss_dur:\u001b[92m 24.638904571533203 \u001b[0m(-0.5488736288888099)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002036571502685547 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 25.512663432529994 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.873758784362248 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 24.638904571533203 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_850.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 17/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:16:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:16:18 -- STEP: 0/50 -- GLOBAL_STEP: 850\u001b[0m\n",
            "     | > loss: 23.22522735595703  (23.22522735595703)\n",
            "     | > log_mle: 0.8820022940635681  (0.8820022940635681)\n",
            "     | > loss_dur: 22.343225479125977  (22.343225479125977)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(20.8167, device='cuda:0')  (tensor(20.8167, device='cuda:0'))\n",
            "     | > current_lr: 4.25e-06 \n",
            "     | > step_time: 2.6943  (2.6943485736846924)\n",
            "     | > loader_time: 4.4734  (4.473425388336182)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0029139178139822824 \u001b[0m(+0.0008773463112967356)\n",
            "     | > avg_loss:\u001b[92m 24.833547047206334 \u001b[0m(-0.6791163853236597)\n",
            "     | > avg_log_mle:\u001b[92m 0.8373908741133553 \u001b[0m(-0.03636791024889263)\n",
            "     | > avg_loss_dur:\u001b[92m 23.99615614754813 \u001b[0m(-0.642748423985072)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0029139178139822824 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 24.833547047206334 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.8373908741133553 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 23.99615614754813 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_900.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 18/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:18:03) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:18:10 -- STEP: 0/50 -- GLOBAL_STEP: 900\u001b[0m\n",
            "     | > loss: 22.639324188232422  (22.639324188232422)\n",
            "     | > log_mle: 0.8474187850952148  (0.8474187850952148)\n",
            "     | > loss_dur: 21.79190444946289  (21.79190444946289)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(20.8461, device='cuda:0')  (tensor(20.8461, device='cuda:0'))\n",
            "     | > current_lr: 4.5e-06 \n",
            "     | > step_time: 2.6839  (2.683933973312378)\n",
            "     | > loader_time: 3.9179  (3.9178946018218994)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0030963761465890066 \u001b[0m(+0.0001824583326067242)\n",
            "     | > avg_loss:\u001b[92m 24.48097120012556 \u001b[0m(-0.35257584708077516)\n",
            "     | > avg_log_mle:\u001b[92m 0.8024392298289708 \u001b[0m(-0.034951644284384575)\n",
            "     | > avg_loss_dur:\u001b[92m 23.678531646728516 \u001b[0m(-0.31762450081961546)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0030963761465890066 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 24.48097120012556 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.8024392298289708 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 23.678531646728516 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_950.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 19/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:19:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:20:04 -- STEP: 0/50 -- GLOBAL_STEP: 950\u001b[0m\n",
            "     | > loss: 22.329814910888672  (22.329814910888672)\n",
            "     | > log_mle: 0.8139922618865967  (0.8139922618865967)\n",
            "     | > loss_dur: 21.515823364257812  (21.515823364257812)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(20.0514, device='cuda:0')  (tensor(20.0514, device='cuda:0'))\n",
            "     | > current_lr: 4.749999999999999e-06 \n",
            "     | > step_time: 1.6998  (1.6997730731964111)\n",
            "     | > loader_time: 3.108  (3.1080286502838135)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0027881009238106863 \u001b[0m(-0.0003082752227783203)\n",
            "     | > avg_loss:\u001b[92m 23.925461087908065 \u001b[0m(-0.5555101122174939)\n",
            "     | > avg_log_mle:\u001b[92m 0.774017470223563 \u001b[0m(-0.028421759605407715)\n",
            "     | > avg_loss_dur:\u001b[92m 23.15144375392369 \u001b[0m(-0.5270878928048255)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0027881009238106863 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 23.925461087908065 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.774017470223563 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 23.15144375392369 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1000.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 20/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:21:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:21:59 -- STEP: 0/50 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > loss: 22.090782165527344  (22.090782165527344)\n",
            "     | > log_mle: 0.7858397364616394  (0.7858397364616394)\n",
            "     | > loss_dur: 21.304943084716797  (21.304943084716797)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(19.7565, device='cuda:0')  (tensor(19.7565, device='cuda:0'))\n",
            "     | > current_lr: 4.9999999999999996e-06 \n",
            "     | > step_time: 1.596  (1.5960395336151123)\n",
            "     | > loader_time: 3.1394  (3.1393513679504395)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0047304630279541016 \u001b[0m(+0.0019423621041434152)\n",
            "     | > avg_loss:\u001b[92m 23.71010180882045 \u001b[0m(-0.21535927908761465)\n",
            "     | > avg_log_mle:\u001b[92m 0.7498538494110107 \u001b[0m(-0.0241636208125523)\n",
            "     | > avg_loss_dur:\u001b[92m 22.96024785723005 \u001b[0m(-0.1911958966936389)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0047304630279541016 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 23.71010180882045 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.7498538494110107 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 22.96024785723005 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1050.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 21/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:23:52) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:23:56 -- STEP: 0/50 -- GLOBAL_STEP: 1050\u001b[0m\n",
            "     | > loss: 21.19182586669922  (21.19182586669922)\n",
            "     | > log_mle: 0.7618227005004883  (0.7618227005004883)\n",
            "     | > loss_dur: 20.430004119873047  (20.430004119873047)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(18.5392, device='cuda:0')  (tensor(18.5392, device='cuda:0'))\n",
            "     | > current_lr: 5.25e-06 \n",
            "     | > step_time: 1.7212  (1.7211618423461914)\n",
            "     | > loader_time: 2.7868  (2.7868077754974365)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0040318284715924945 \u001b[0m(-0.000698634556361607)\n",
            "     | > avg_loss:\u001b[92m 22.64527702331543 \u001b[0m(-1.0648247855050208)\n",
            "     | > avg_log_mle:\u001b[92m 0.7294573528426034 \u001b[0m(-0.020396496568407363)\n",
            "     | > avg_loss_dur:\u001b[92m 21.915819576808385 \u001b[0m(-1.0444282804216662)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0040318284715924945 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 22.64527702331543 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.7294573528426034 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 21.915819576808385 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1100.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 22/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:25:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:25:53 -- STEP: 0/50 -- GLOBAL_STEP: 1100\u001b[0m\n",
            "     | > loss: 20.7684268951416  (20.7684268951416)\n",
            "     | > log_mle: 0.741117000579834  (0.741117000579834)\n",
            "     | > loss_dur: 20.02730941772461  (20.02730941772461)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.8850, device='cuda:0')  (tensor(17.8850, device='cuda:0'))\n",
            "     | > current_lr: 5.5e-06 \n",
            "     | > step_time: 1.3912  (1.3912391662597656)\n",
            "     | > loader_time: 3.399  (3.3989784717559814)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0019412040710449219 \u001b[0m(-0.0020906244005475727)\n",
            "     | > avg_loss:\u001b[92m 22.248817443847656 \u001b[0m(-0.39645957946777344)\n",
            "     | > avg_log_mle:\u001b[92m 0.7112479380198887 \u001b[0m(-0.018209414822714654)\n",
            "     | > avg_loss_dur:\u001b[92m 21.537569727216447 \u001b[0m(-0.3782498495919384)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0019412040710449219 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 22.248817443847656 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.7112479380198887 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 21.537569727216447 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1150.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 23/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:27:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:27:49 -- STEP: 0/50 -- GLOBAL_STEP: 1150\u001b[0m\n",
            "     | > loss: 19.77836799621582  (19.77836799621582)\n",
            "     | > log_mle: 0.721882164478302  (0.721882164478302)\n",
            "     | > loss_dur: 19.056486129760742  (19.056486129760742)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(16.9763, device='cuda:0')  (tensor(16.9763, device='cuda:0'))\n",
            "     | > current_lr: 5.75e-06 \n",
            "     | > step_time: 1.41  (1.410020112991333)\n",
            "     | > loader_time: 3.233  (3.2330286502838135)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0028713090079171316 \u001b[0m(+0.0009301049368722098)\n",
            "     | > avg_loss:\u001b[92m 21.772479466029576 \u001b[0m(-0.47633797781807985)\n",
            "     | > avg_log_mle:\u001b[92m 0.6939547913415092 \u001b[0m(-0.017293146678379556)\n",
            "     | > avg_loss_dur:\u001b[92m 21.078524317060197 \u001b[0m(-0.45904541015625)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0028713090079171316 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.772479466029576 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6939547913415092 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 21.078524317060197 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1200.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 24/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:29:43) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:29:48 -- STEP: 0/50 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > loss: 19.964017868041992  (19.964017868041992)\n",
            "     | > log_mle: 0.7042320966720581  (0.7042320966720581)\n",
            "     | > loss_dur: 19.25978660583496  (19.25978660583496)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.6325, device='cuda:0')  (tensor(17.6325, device='cuda:0'))\n",
            "     | > current_lr: 6e-06 \n",
            "     | > step_time: 1.5987  (1.5987293720245361)\n",
            "     | > loader_time: 3.3079  (3.3078813552856445)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.001985720225742885 \u001b[0m(-0.0008855887821742465)\n",
            "     | > avg_loss:\u001b[92m 21.60244928087507 \u001b[0m(-0.17003018515450563)\n",
            "     | > avg_log_mle:\u001b[92m 0.6776637434959412 \u001b[0m(-0.016291047845568007)\n",
            "     | > avg_loss_dur:\u001b[92m 20.924785614013672 \u001b[0m(-0.1537387030465247)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.001985720225742885 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.60244928087507 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6776637434959412 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.924785614013672 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1250.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 25/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:31:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:31:45 -- STEP: 0/50 -- GLOBAL_STEP: 1250\u001b[0m\n",
            "     | > loss: 19.916976928710938  (19.916976928710938)\n",
            "     | > log_mle: 0.6872297525405884  (0.6872297525405884)\n",
            "     | > loss_dur: 19.229747772216797  (19.229747772216797)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.7270, device='cuda:0')  (tensor(17.7270, device='cuda:0'))\n",
            "     | > current_lr: 6.2499999999999995e-06 \n",
            "     | > step_time: 1.5703  (1.570336103439331)\n",
            "     | > loader_time: 3.2107  (3.210742712020874)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0033090455191476004 \u001b[0m(+0.0013233252934047152)\n",
            "     | > avg_loss:\u001b[91m 21.921932765415736 \u001b[0m(+0.31948348454066533)\n",
            "     | > avg_log_mle:\u001b[92m 0.6630742635045733 \u001b[0m(-0.014589479991367837)\n",
            "     | > avg_loss_dur:\u001b[91m 21.25885854448591 \u001b[0m(+0.33407293047223874)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0033090455191476004 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.921932765415736 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6630742635045733 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 21.25885854448591 \u001b[0m(+0.0)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 26/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:33:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:33:40 -- STEP: 0/50 -- GLOBAL_STEP: 1300\u001b[0m\n",
            "     | > loss: 19.470407485961914  (19.470407485961914)\n",
            "     | > log_mle: 0.6723793745040894  (0.6723793745040894)\n",
            "     | > loss_dur: 18.79802894592285  (18.79802894592285)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.3615, device='cuda:0')  (tensor(17.3615, device='cuda:0'))\n",
            "     | > current_lr: 6.5e-06 \n",
            "     | > step_time: 1.5487  (1.548712968826294)\n",
            "     | > loader_time: 3.8499  (3.84985613822937)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0027866363525390625 \u001b[0m(-0.0005224091666085379)\n",
            "     | > avg_loss:\u001b[92m 21.5941002709525 \u001b[0m(-0.32783249446323737)\n",
            "     | > avg_log_mle:\u001b[92m 0.6495065774236407 \u001b[0m(-0.013567686080932617)\n",
            "     | > avg_loss_dur:\u001b[92m 20.94459342956543 \u001b[0m(-0.3142651149204809)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0027866363525390625 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.5941002709525 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6495065774236407 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.94459342956543 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1350.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 27/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:35:27) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:35:34 -- STEP: 0/50 -- GLOBAL_STEP: 1350\u001b[0m\n",
            "     | > loss: 18.797225952148438  (18.797225952148438)\n",
            "     | > log_mle: 0.6587718725204468  (0.6587718725204468)\n",
            "     | > loss_dur: 18.13845443725586  (18.13845443725586)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(16.8769, device='cuda:0')  (tensor(16.8769, device='cuda:0'))\n",
            "     | > current_lr: 6.75e-06 \n",
            "     | > step_time: 2.3185  (2.3185465335845947)\n",
            "     | > loader_time: 4.8383  (4.838254928588867)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0020212786538260324 \u001b[0m(-0.0007653576987130301)\n",
            "     | > avg_loss:\u001b[92m 21.372795786176408 \u001b[0m(-0.22130448477609121)\n",
            "     | > avg_log_mle:\u001b[92m 0.6356446487562997 \u001b[0m(-0.013861928667341039)\n",
            "     | > avg_loss_dur:\u001b[92m 20.737151418413436 \u001b[0m(-0.20744201115199346)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0020212786538260324 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.372795786176408 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6356446487562997 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.737151418413436 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1400.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 28/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:37:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:37:28 -- STEP: 0/50 -- GLOBAL_STEP: 1400\u001b[0m\n",
            "     | > loss: 18.983631134033203  (18.983631134033203)\n",
            "     | > log_mle: 0.6443922519683838  (0.6443922519683838)\n",
            "     | > loss_dur: 18.3392391204834  (18.3392391204834)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.3411, device='cuda:0')  (tensor(17.3411, device='cuda:0'))\n",
            "     | > current_lr: 7e-06 \n",
            "     | > step_time: 2.7281  (2.72812557220459)\n",
            "     | > loader_time: 5.2896  (5.289614200592041)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0034167425973074777 \u001b[0m(+0.0013954639434814453)\n",
            "     | > avg_loss:\u001b[92m 21.170677866254533 \u001b[0m(-0.202117919921875)\n",
            "     | > avg_log_mle:\u001b[92m 0.6215761133602687 \u001b[0m(-0.014068535396030923)\n",
            "     | > avg_loss_dur:\u001b[92m 20.54910182952881 \u001b[0m(-0.18804958888462764)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0034167425973074777 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.170677866254533 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6215761133602687 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.54910182952881 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1450.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 29/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:39:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:39:21 -- STEP: 0/50 -- GLOBAL_STEP: 1450\u001b[0m\n",
            "     | > loss: 19.025936126708984  (19.025936126708984)\n",
            "     | > log_mle: 0.6296718716621399  (0.6296718716621399)\n",
            "     | > loss_dur: 18.396265029907227  (18.396265029907227)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.6038, device='cuda:0')  (tensor(17.6038, device='cuda:0'))\n",
            "     | > current_lr: 7.25e-06 \n",
            "     | > step_time: 2.5732  (2.573183298110962)\n",
            "     | > loader_time: 3.2951  (3.2950596809387207)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002143587384905134 \u001b[0m(-0.0012731552124023438)\n",
            "     | > avg_loss:\u001b[92m 21.00169508797782 \u001b[0m(-0.16898277827671393)\n",
            "     | > avg_log_mle:\u001b[92m 0.6082749537059239 \u001b[0m(-0.013301159654344863)\n",
            "     | > avg_loss_dur:\u001b[92m 20.393420219421387 \u001b[0m(-0.15568161010742188)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002143587384905134 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 21.00169508797782 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.6082749537059239 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.393420219421387 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1500.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 30/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:41:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:41:17 -- STEP: 0/50 -- GLOBAL_STEP: 1500\u001b[0m\n",
            "     | > loss: 18.707538604736328  (18.707538604736328)\n",
            "     | > log_mle: 0.616034746170044  (0.616034746170044)\n",
            "     | > loss_dur: 18.091503143310547  (18.091503143310547)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.5875, device='cuda:0')  (tensor(17.5875, device='cuda:0'))\n",
            "     | > current_lr: 7.499999999999999e-06 \n",
            "     | > step_time: 1.6119  (1.6118826866149902)\n",
            "     | > loader_time: 3.1877  (3.187739610671997)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0034102031162806918 \u001b[0m(+0.0012666157313755578)\n",
            "     | > avg_loss:\u001b[92m 20.773873601640975 \u001b[0m(-0.2278214863368433)\n",
            "     | > avg_log_mle:\u001b[92m 0.5952604583331517 \u001b[0m(-0.013014495372772217)\n",
            "     | > avg_loss_dur:\u001b[92m 20.17861352648054 \u001b[0m(-0.2148066929408472)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0034102031162806918 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 20.773873601640975 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5952604583331517 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.17861352648054 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1550.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 31/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:43:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:43:14 -- STEP: 0/50 -- GLOBAL_STEP: 1550\u001b[0m\n",
            "     | > loss: 18.428081512451172  (18.428081512451172)\n",
            "     | > log_mle: 0.6026500463485718  (0.6026500463485718)\n",
            "     | > loss_dur: 17.82543182373047  (17.82543182373047)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.5809, device='cuda:0')  (tensor(17.5809, device='cuda:0'))\n",
            "     | > current_lr: 7.75e-06 \n",
            "     | > step_time: 1.6522  (1.6521532535552979)\n",
            "     | > loader_time: 2.9828  (2.982764959335327)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0027600015912737164 \u001b[0m(-0.0006502015250069754)\n",
            "     | > avg_loss:\u001b[92m 20.62424741472517 \u001b[0m(-0.14962618691580687)\n",
            "     | > avg_log_mle:\u001b[92m 0.5822010380881173 \u001b[0m(-0.01305942024503437)\n",
            "     | > avg_loss_dur:\u001b[92m 20.042046819414413 \u001b[0m(-0.13656670706612672)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0027600015912737164 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 20.62424741472517 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5822010380881173 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 20.042046819414413 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1600.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 32/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:45:10) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:45:16 -- STEP: 0/50 -- GLOBAL_STEP: 1600\u001b[0m\n",
            "     | > loss: 18.192502975463867  (18.192502975463867)\n",
            "     | > log_mle: 0.5888771414756775  (0.5888771414756775)\n",
            "     | > loss_dur: 17.603626251220703  (17.603626251220703)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.6252, device='cuda:0')  (tensor(17.6252, device='cuda:0'))\n",
            "     | > current_lr: 8e-06 \n",
            "     | > step_time: 2.5302  (2.530214548110962)\n",
            "     | > loader_time: 3.337  (3.3370425701141357)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.003051689692905971 \u001b[0m(+0.0002916881016322547)\n",
            "     | > avg_loss:\u001b[92m 20.44875376565116 \u001b[0m(-0.17549364907401)\n",
            "     | > avg_log_mle:\u001b[92m 0.5688356672014508 \u001b[0m(-0.01336537088666645)\n",
            "     | > avg_loss_dur:\u001b[92m 19.879918234688894 \u001b[0m(-0.16212858472551872)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.003051689692905971 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 20.44875376565116 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5688356672014508 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 19.879918234688894 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1650.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 33/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:47:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:47:12 -- STEP: 0/50 -- GLOBAL_STEP: 1650\u001b[0m\n",
            "     | > loss: 18.016759872436523  (18.016759872436523)\n",
            "     | > log_mle: 0.5745666027069092  (0.5745666027069092)\n",
            "     | > loss_dur: 17.44219398498535  (17.44219398498535)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.5651, device='cuda:0')  (tensor(17.5651, device='cuda:0'))\n",
            "     | > current_lr: 8.25e-06 \n",
            "     | > step_time: 1.5936  (1.5936200618743896)\n",
            "     | > loader_time: 3.15  (3.150012493133545)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002897228513445173 \u001b[0m(-0.00015446117946079805)\n",
            "     | > avg_loss:\u001b[92m 20.3958192552839 \u001b[0m(-0.05293451036725827)\n",
            "     | > avg_log_mle:\u001b[92m 0.5553787691252572 \u001b[0m(-0.013456898076193657)\n",
            "     | > avg_loss_dur:\u001b[92m 19.840440613882883 \u001b[0m(-0.039477620806010805)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002897228513445173 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 20.3958192552839 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5553787691252572 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 19.840440613882883 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1700.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 34/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:49:05) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:49:10 -- STEP: 0/50 -- GLOBAL_STEP: 1700\u001b[0m\n",
            "     | > loss: 17.149595260620117  (17.149595260620117)\n",
            "     | > log_mle: 0.5604512691497803  (0.5604512691497803)\n",
            "     | > loss_dur: 16.589143753051758  (16.589143753051758)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(16.9465, device='cuda:0')  (tensor(16.9465, device='cuda:0'))\n",
            "     | > current_lr: 8.5e-06 \n",
            "     | > step_time: 1.5566  (1.556579351425171)\n",
            "     | > loader_time: 3.3806  (3.3805744647979736)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0025878974369594027 \u001b[0m(-0.00030933107648577034)\n",
            "     | > avg_loss:\u001b[92m 20.125878061567033 \u001b[0m(-0.26994119371686764)\n",
            "     | > avg_log_mle:\u001b[92m 0.5418181163924081 \u001b[0m(-0.013560652732849121)\n",
            "     | > avg_loss_dur:\u001b[92m 19.584060260227748 \u001b[0m(-0.25638035365513545)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0025878974369594027 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 20.125878061567033 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5418181163924081 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 19.584060260227748 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1750.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 35/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:51:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:51:09 -- STEP: 0/50 -- GLOBAL_STEP: 1750\u001b[0m\n",
            "     | > loss: 17.501638412475586  (17.501638412475586)\n",
            "     | > log_mle: 0.5456752777099609  (0.5456752777099609)\n",
            "     | > loss_dur: 16.955963134765625  (16.955963134765625)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.4366, device='cuda:0')  (tensor(17.4366, device='cuda:0'))\n",
            "     | > current_lr: 8.750000000000001e-06 \n",
            "     | > step_time: 1.8203  (1.8202519416809082)\n",
            "     | > loader_time: 3.0282  (3.0281548500061035)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002168893814086914 \u001b[0m(-0.00041900362287248865)\n",
            "     | > avg_loss:\u001b[92m 19.864961624145508 \u001b[0m(-0.2609164374215247)\n",
            "     | > avg_log_mle:\u001b[92m 0.5284098386764526 \u001b[0m(-0.01340827771595543)\n",
            "     | > avg_loss_dur:\u001b[92m 19.336551666259766 \u001b[0m(-0.2475085939679822)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002168893814086914 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 19.864961624145508 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5284098386764526 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 19.336551666259766 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1800.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 36/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:53:02) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:53:07 -- STEP: 0/50 -- GLOBAL_STEP: 1800\u001b[0m\n",
            "     | > loss: 17.19863510131836  (17.19863510131836)\n",
            "     | > log_mle: 0.5312339663505554  (0.5312339663505554)\n",
            "     | > loss_dur: 16.667400360107422  (16.667400360107422)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.4063, device='cuda:0')  (tensor(17.4063, device='cuda:0'))\n",
            "     | > current_lr: 9e-06 \n",
            "     | > step_time: 1.3941  (1.3941481113433838)\n",
            "     | > loader_time: 3.4054  (3.4054460525512695)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.002027273178100586 \u001b[0m(-0.00014162063598632812)\n",
            "     | > avg_loss:\u001b[92m 19.607156617300852 \u001b[0m(-0.2578050068446558)\n",
            "     | > avg_log_mle:\u001b[92m 0.5158722570964268 \u001b[0m(-0.012537581580025825)\n",
            "     | > avg_loss_dur:\u001b[92m 19.091284343174525 \u001b[0m(-0.24526732308524046)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.002027273178100586 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 19.607156617300852 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5158722570964268 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 19.091284343174525 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1850.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 37/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:55:01) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:55:06 -- STEP: 0/50 -- GLOBAL_STEP: 1850\u001b[0m\n",
            "     | > loss: 16.851308822631836  (16.851308822631836)\n",
            "     | > log_mle: 0.5172572135925293  (0.5172572135925293)\n",
            "     | > loss_dur: 16.33405113220215  (16.33405113220215)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.3581, device='cuda:0')  (tensor(17.3581, device='cuda:0'))\n",
            "     | > current_lr: 9.250000000000001e-06 \n",
            "     | > step_time: 1.6207  (1.620734691619873)\n",
            "     | > loader_time: 3.0449  (3.0448801517486572)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0029166085379464285 \u001b[0m(+0.0008893353598458426)\n",
            "     | > avg_loss:\u001b[92m 19.321101188659668 \u001b[0m(-0.28605542864118405)\n",
            "     | > avg_log_mle:\u001b[92m 0.5044238695076534 \u001b[0m(-0.011448387588773423)\n",
            "     | > avg_loss_dur:\u001b[92m 18.816677365984237 \u001b[0m(-0.27460697719028815)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0029166085379464285 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 19.321101188659668 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.5044238695076534 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 18.816677365984237 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1900.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 38/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:57:00) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:57:05 -- STEP: 0/50 -- GLOBAL_STEP: 1900\u001b[0m\n",
            "     | > loss: 16.14384651184082  (16.14384651184082)\n",
            "     | > log_mle: 0.504236102104187  (0.504236102104187)\n",
            "     | > loss_dur: 15.63961124420166  (15.63961124420166)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(16.6420, device='cuda:0')  (tensor(16.6420, device='cuda:0'))\n",
            "     | > current_lr: 9.499999999999999e-06 \n",
            "     | > step_time: 1.5708  (1.5708434581756592)\n",
            "     | > loader_time: 3.3753  (3.375300645828247)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.003935609545026507 \u001b[0m(+0.0010190010070800786)\n",
            "     | > avg_loss:\u001b[92m 19.032914297921316 \u001b[0m(-0.288186890738352)\n",
            "     | > avg_log_mle:\u001b[92m 0.49319384353501455 \u001b[0m(-0.011230025972638835)\n",
            "     | > avg_loss_dur:\u001b[92m 18.539720262799943 \u001b[0m(-0.27695710318429434)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.003935609545026507 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 19.032914297921316 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.49319384353501455 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 18.539720262799943 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_1950.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 39/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 00:58:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 00:59:03 -- STEP: 0/50 -- GLOBAL_STEP: 1950\u001b[0m\n",
            "     | > loss: 16.12113380432129  (16.12113380432129)\n",
            "     | > log_mle: 0.4928693175315857  (0.4928693175315857)\n",
            "     | > loss_dur: 15.628265380859375  (15.628265380859375)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.0339, device='cuda:0')  (tensor(17.0339, device='cuda:0'))\n",
            "     | > current_lr: 9.75e-06 \n",
            "     | > step_time: 1.6721  (1.6720740795135498)\n",
            "     | > loader_time: 3.1406  (3.140556812286377)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.0021690300532749723 \u001b[0m(-0.0017665794917515348)\n",
            "     | > avg_loss:\u001b[92m 18.8566677910941 \u001b[0m(-0.17624650682721565)\n",
            "     | > avg_log_mle:\u001b[92m 0.48236291323389324 \u001b[0m(-0.010830930301121311)\n",
            "     | > avg_loss_dur:\u001b[92m 18.374304907662527 \u001b[0m(-0.1654153551374158)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0021690300532749723 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 18.8566677910941 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.48236291323389324 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 18.374304907662527 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_2000.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 40/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 01:00:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 01:01:02 -- STEP: 0/50 -- GLOBAL_STEP: 2000\u001b[0m\n",
            "     | > loss: 15.78085994720459  (15.78085994720459)\n",
            "     | > log_mle: 0.48144271969795227  (0.48144271969795227)\n",
            "     | > loss_dur: 15.299417495727539  (15.299417495727539)\n",
            "     | > amp_scaler: 4096.0  (4096.0)\n",
            "     | > grad_norm: tensor(17.0037, device='cuda:0')  (tensor(17.0037, device='cuda:0'))\n",
            "     | > current_lr: 9.999999999999999e-06 \n",
            "     | > step_time: 1.4412  (1.4411780834197998)\n",
            "     | > loader_time: 3.4323  (3.4323296546936035)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.0029793807438441683 \u001b[0m(+0.0008103506905691961)\n",
            "     | > avg_loss:\u001b[92m 18.617325919015066 \u001b[0m(-0.23934187207903435)\n",
            "     | > avg_log_mle:\u001b[92m 0.47254463178770884 \u001b[0m(-0.009818281446184407)\n",
            "     | > avg_loss_dur:\u001b[92m 18.144781521388463 \u001b[0m(-0.22952338627406377)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.0029793807438441683 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 18.617325919015066 \u001b[0m(+0.0)\n",
            "     | > avg_log_mle: 0.47254463178770884 \u001b[0m(+0.0)\n",
            "     | > avg_loss_dur: 18.144781521388463 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/output/run-August-12-2023_11+43PM-0000000/best_model_2050.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 41/100\u001b[0m\n",
            " --> /content/output/run-August-12-2023_11+43PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-08-13 01:02:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-08-13 01:02:59 -- STEP: 0/50 -- GLOBAL_STEP: 2050\u001b[0m\n",
            "     | > loss: 15.199819564819336  (15.199819564819336)\n",
            "     | > log_mle: 0.4708048105239868  (0.4708048105239868)\n",
            "     | > loss_dur: 14.72901439666748  (14.72901439666748)\n",
            "     | > amp_scaler: 8192.0  (8192.0)\n",
            "     | > grad_norm: tensor(16.4805, device='cuda:0')  (tensor(16.4805, device='cuda:0'))\n",
            "     | > current_lr: 1.025e-05 \n",
            "     | > step_time: 1.5396  (1.5395569801330566)\n",
            "     | > loader_time: 3.2339  (3.2339329719543457)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}